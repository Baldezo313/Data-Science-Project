{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6daOGuc4KPS"
   },
   "source": [
    "# ***Classification de Texte à l'aide de Scikit-learn,python et NLTK***   \n",
    "\n",
    "***Classification des documents/Texte*** est l'une des tâches importantes et typique de *supervisé* du Machine learning. L'attribution de catégories à des documents, qui peut être une page Web, un livre de bibliothèque, des articles médiatiques, une galerie, etc. a de nombreuses applications telles que le filtrage anti-spam, le routage des e-mails, l'analyse des sentiments etc. classification en utilisant python, scikit-learn et un peu de NLTK.  \n",
    "   \n",
    "Divisons le problème de classification en étapes ci-dessous:  \n",
    "1. Prérequis et mise en place de l'environnement.  \n",
    "2. Chargement de l'ensemble de données dans jupyter.  \n",
    "3. Extraction de fonctionnalités à partir de fichiers texte.  \n",
    "4. Exécution d'algorithmes de ML.  \n",
    "5. Grille Recherche pour le réglage des paramètres.  \n",
    "6. Conseils utiles et une touche de NLTK.  \n",
    "   \n",
    "***Étape 1: Prérequis et mise en place de l'environnement***  \n",
    "Les prérequis pour suivre cet exemple sont python version 2.7.3 et jupyter notebook. Vous pouvez simplement installer anaconda et il aura tout pour vous. En outre, un peu de bases de python et de ML, y compris la classification de texte, est nécessaire. Nous utiliserons les bibliothèques scikit-learn (python) pour notre exemple.  \n",
    "   \n",
    "***Étape 2: chargement de l'ensemble de données dans jupyter.***  \n",
    "Le jeu de données utilisé pour cet exemple est le fameux jeu de données \"20 Newsgroup\".   \n",
    "L'ensemble de données des 20 groupes de discussion est une collection d'environ 20000 documents de groupes de discussion, répartis(presque) uniformément sur 20 groupes de discussion différents. A ma connaissance, il a été collecté à l'origine par *Ken Lang*, probablement pour son *Newsweeder:Apprendre à filtrer* le papier de *netnews, bien qu'il ne mentionne pas explicitement cette collection. La collection de 20 groupes de discussion est devenue un ensemble de données populaire pour les expériences dans les applications textuelles des techniques d'apprentissage automatique, telles que la classification de texte et le clustering de texte.  \n",
    "   \n",
    "Cet ensemble de données est intégré à scikit, nous n'avons donc pas besoin de le télécharger explicitement.  \n",
    "  \n",
    "**chargement de l'ensemble de données**  \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyTcVtQB4AeA",
    "outputId": "75c7b2d8-e1af-4f12-f15b-fea3a5a4fbd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset -training data.  \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "\n",
    "# we can check the target names (categories) and some data files by following commands.  \n",
    "twenty_train.target_names       # prints all the categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0jX7f464Hyq"
   },
   "source": [
    "Remarque: ci-dessus, nous ne chargeons que les données d' entraînement . Nous chargerons les données de test séparément plus tard dans l'exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WibqbncweO0s"
   },
   "source": [
    "On peut vérifier les noms des cibles (catégories) et certains fichiers de données en suivant les commandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5ptOIsyd-7_",
    "outputId": "4a79377b-44e0-48e4-8211-eedeefb9bb49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))    # prints first line of the first data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUB-xKpXe7Hs"
   },
   "source": [
    "***Étape 3: extraction de fonctionnalités à partir de fichiers texte***  \n",
    "  \n",
    "Les fichiers texte sont en fait des séries de mots (ordonnés). Afin d'exécuter des algorithmes d'apprentissage automatique, nous devons convertir les fichiers texte en vecteurs de caractéristiques numériques. Nous utiliserons le modèle **du sac de mots** pour notre exemple. En bref, nous segmentons chaque fichier texte en mots (pour le fractionnement anglais par espace), et comptons # de fois que chaque mot apparaît dans chaque document et attribuons finalement à chaque mot un identifiant entier. **Chaque mot unique de notre dictionnaire correspondra à une caractéristique (caractéristique descriptive).**  \n",
    "   \n",
    "Scikit-learn a un composant de haut niveau qui créera des vecteurs de caractéristiques pour nous 'CountVectorizer'\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6f3wVoHexxr",
    "outputId": "bcea0b76-f26e-4e4a-90d0-0f5dddd00ad4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting features from text files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EN91VIl4g1-r"
   },
   "source": [
    "Ici en faisant 'count_vect.fit_transform(twenty_train.data)', nous apprenons le dictionnaire de vocabulaire et il renvoie une matrice de Document-Term. [n_samples, n_features]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xxwm0o7vhgjp"
   },
   "source": [
    "**TF:** Le simple fait de compter le nombre de mots dans chaque document a un problème:cela donnera plus de poids aux documents plus longs qu'aux documents plus courts. Pour éviter cela, nous pouvons utiliser la fréquence (**TF (Term Frequencies)**) ie #count(word)/ #Total words, dans chaque document.  \n",
    "  \n",
    "**TF-IDF**: Enfin, nous pouvons même réduire le poids de mots plus courants comme (the,is,an,etc) qui apparaissent dans tous les documents. Ceci est appelé **TF-IDF, c'est-à-dire la fréquence des termes multipliéé par fréquence inverse du document.**   \n",
    "   \n",
    "Nous pouvons réaliser les deux en utilisant la ligne de code ci-dessous:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqtmvSIAgt8j",
    "outputId": "21d76c4c-a0ce-483e-acf4-55676a80c67e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IFD \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wO3gJWY26mXi"
   },
   "source": [
    "***Étape 4. Exécution d'algorithmes de ML***  \n",
    "  \n",
    "Ilexiste differents algorithmes qui peuvent être utilisés pour la classification de text. Nous allons commencer par le plus simple 'Naive Bayes(NB)'  \n",
    "   \n",
    "On peut facilement créer un NBclassifier dans scikit en utilisant ci-dessous 2 lignes de code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9UDLTzM6exy",
    "outputId": "fbdd82a8-67d4-46fd-f973-9d44536dfdfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Machine Learning\n",
    "# Training Naive Bayes (NB) classifier on traininig data\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qpmOy_97y6_"
   },
   "source": [
    "Cela formera le classificateur NB sur les données de formation que nous avons fournies.  \n",
    "\n",
    "**Construire un pipeline:nous pouvons écrire moins de code et faire tout ce  qui précède, en construisant un pipeline comme suit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NC-gegkH7rm_"
   },
   "outputs": [],
   "source": [
    "# Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "# The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "# We will be using the 'text_clf' going forward.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dgbXAWT9Ph3"
   },
   "source": [
    "Les nom \"vect\", \"tfidf\" et \"clf\" sont arbitraires mais seront utilisés plus tard.  \n",
    "   \n",
    "\n",
    "**Performance du classificateur NB**: Nous allons maintenant tester les performances du classificateur NB sur **l'ensemble de test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMLkVYlg9E4c",
    "outputId": "d6a5f7c5-532d-4659-e88f-28224d9f00fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance of NB Classifier\n",
    "import numpy as np\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ppip3bfb-YXM"
   },
   "source": [
    "La précision que nous obtenons est de **~77.38%**, ce qui n'est pas mal pour le début et pour un classificateur naïf. \n",
    "   \n",
    "***Support Vector Machine(SVM)***:Essayons d'utiliser un autre algorithme SVM, et voyons si nous pouvons obtenir de meilleures performances.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNNslo_c-RsE",
    "outputId": "232fd43b-1e29-4d13-bf42-c87d829ec035"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8248805098247477"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Support Vector Machines - SVM and calculating its performance\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf-svm', SGDClassifier(loss='hinge',\n",
    "                                                                                                               penalty='l2', alpha=1e-3,\n",
    "                                                                                                               max_iter=5, random_state=42))])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "\n",
    "np.mean(predicted_svm == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HeWC4D_BOPw"
   },
   "source": [
    "La précision que nous obtenons est de **~82.38%**. Yipee, un peu mieux  \n",
    "   \n",
    "***Étape 5. Recherche de grille***  \n",
    "Presque tous les classificateurs auront divers paramètres qui peuvent être réglés pour obtenir des performances optimales. Scikit propose un outil extrêmement utile \"GridSearchCV\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LEAh-GoKAbx6"
   },
   "outputs": [],
   "source": [
    "# Grid SEARCH\n",
    "# Here, we are creating a list of parameters for which we would like to do performance tuning.  \n",
    "# All the parameters name start with the classifier name (remember the arbitrary name we gave). \n",
    "# E.g. vect_ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.  \n",
    "  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1,1), (1,2)],\n",
    "              'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JSOt9JEE_5w"
   },
   "source": [
    "Ici, nous créons une liste de paramètres pour lesquels nous aimerions faire un réglage des performances. Tous les noms de paramètres commencent par le nom du classificateur.  \n",
    "   \n",
    "Ensuite, nous créons une instance de la recherche de grille en passant le classificateur, les paramètres et n_jobs=-1 qui indique d'utiliser plusieurs coeurs de la machine utilisateur.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgeiI_MKE7xL",
    "outputId": "841c2937-0cb2-4999-adf4-374eabb6355f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0ALNJ7nIoXs"
   },
   "source": [
    "Enfin, pour voir le meilleur score moyen et les paramètres, exécutons le code suivant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lul5f_WgHAXG",
    "outputId": "eaf70eae-5e2d-409d-9931-14a413cc45e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('vect',\n",
      "                 CountVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
      "                                 input='content', lowercase=True, max_df=1.0,\n",
      "                                 max_features=None, min_df=1,\n",
      "                                 ngram_range=(1, 2), preprocessor=None,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, vocabulary=None)),\n",
      "                ('tfidf',\n",
      "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
      "                                  sublinear_tf=False, use_idf=True)),\n",
      "                ('clf',\n",
      "                 MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True))],\n",
      "         verbose=False)\n",
      "{'clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "print(gs_clf.best_estimator_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DTt5-4_SWmrQ",
    "outputId": "1c2748e1-38da-4221-a06a-93312d67e111"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8361656930430165"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gs_clf_predicted = gs_clf.predict(twenty_test.data)\n",
    "np.mean(gs_clf_predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5IBpLkLQvJS"
   },
   "source": [
    "La précision a maitenant auguementé à **~84.6% pour le classificateur NB** (plus naïf! Anymore) et les paramètres correspondants sont {'clf__alpha':0.01,'tfidf__use_idf':True,'vect__ngram_range':(1,2)}.   \n",
    "   \n",
    "De même, nous obtenons une précision améliorée d'**environ 83.79%** pour le classificateur SVM avec le code ci-dessous. (On peut optimiser davantage le classificateur SVM en réglant d'autres paramètres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANTsjrVeNkMs",
    "outputId": "9f7fa810-a8aa-4c51-f4be-e3f9f31b970f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf-svm__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarly doing grid search for SVM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'clf-svm__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PR3TJ0tDXuxy"
   },
   "outputs": [],
   "source": [
    "gs_clf_svm_predicted = gs_clf_svm.predict(twenty_test.data)\n",
    "np.mean(gs_clf_svm_predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfxCeOD2Spw5"
   },
   "source": [
    "***Étape 6: Conseils utiles et une touche de NLTK***  \n",
    "  \n",
    "1. **Suppression des mots vides** : (le,puis,etc.) des données. Vous ne devez le faire que lorsque les mots vides ne sont pas utiles pour le problème sous-jacent. Dans la plupart des problèmes de classification de text, cela n'est en effet pas utile. Voyons si la suppression des mots vides augumente la précision. Mettez à jour le code de création d'objet de CountVectorizer comme suit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fmz2c6qzSkkK"
   },
   "outputs": [],
   "source": [
    "# NLTK\n",
    "# Removing stop words\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2KpIuJKaeyV"
   },
   "source": [
    "C'est le pipeline que nous construisons pour le classificateur NB. Exécutez les étapes restantes comme avant. Cela améliore la précision de 77,38% à 81,69% (c'est trop bon). Vous pouvez essayer la même chose pour SVM et également en effectuant une recherche dans la grille."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Vx55ws_ay_I"
   },
   "source": [
    "Nous avons besoin de NLTK qui est livré avec divers souches qui peuvent aider à réduire les mots à leur forme racine.  \n",
    "  \n",
    "Ci-dessous, on a utilisé le \"stemmer Snowball\" qui fonctionne très bien pour la langue anglaise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFukX9vTVb1A"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),\n",
    "...                      ('tfidf', TfidfTransformer()),\n",
    "...                      ('mnb', MultinomialNB(fit_prior=False)),\n",
    "... ])\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)\n",
    "np.mean(predicted_mnb_stemmed == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kAUrjKLbic4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Machine_Learning, NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
