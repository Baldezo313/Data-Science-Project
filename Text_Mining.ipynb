{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KRZ2rUZd7xE"
   },
   "source": [
    "# ***Text Mining in Python: Steps and Examples***   \n",
    "Dans le scénario d'aujourd'hui, un moyen de réussir les gens est identifié par la façon dont ils communiquent et partagent des informations avec les autres. C'est là que les concepts de langage entrent en jeu. Cependant, il existe de nombreuses langues dans le monde. Chacun a de nombreux standards et alphabets, et la combinaison de ces mots disposés de manière significative a abouti à la formation d'une phrase. Chaque langue a ses propres règles lors du développement de ces phrases et ces ensembles de règles sont également connus sous le nom de grammaire.\n",
    "   \n",
    "Dans le monde d'aujourd'hui, selon les estimations de l'industrie, seulement 20% des données sont générées au format structuré au moment où nous parlons, lorsque nous tweetons, lorsque nous envoyons des messages sur WhatsApp, e-mail, Facebook, Instagram ou tout autre message texte. Et la majorité de ces données existe sous forme textuelle qui est un format hautement non structuré. Afin de produire des informations significatives à partir des données de texte, nous devons suivre une méthode appelée Analyse de texte.  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRzQAJmzegf3"
   },
   "source": [
    "## ***What is Text Mining?***  \n",
    "   \n",
    "L'exploration de texte est le processus de dérivation d'informations significatives à partir d'un texte en langage naturel.  \n",
    "   \n",
    "***What is NLP?***  \n",
    "*Le traitement du langage naturel (NLP) fait partie de l'informatique et de l'intelligence artificielle qui traite des langues humaines.*   \n",
    "   \n",
    "En d'autres termes, la PNL est un composant de l'exploration de texte qui effectue un type spécial d'analyse linguistique qui aide essentiellement une machine à «lire» le texte . Il utilise une méthodologie différente pour déchiffrer les ambiguïtés dans le langage humain , y compris les suivantes: résumé automatique, marquage d'une partie du discours, désambiguïsation, découpage, ainsi que désambiguïsation et compréhension et reconnaissance du langage naturel. Nous verrons tous les processus étape par étape en utilisant Python.   \n",
    "   \n",
    "Tout d'abord, nous devons installer la bibliothèque NLTK qui est la boîte à outils en langage naturel pour créer des programmes Python pour travailler avec des données en langage humain et qui fournit également une interface facile à utiliser.   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wZMsRswfS7j"
   },
   "source": [
    "## ***Terminologies in NLP***  \n",
    "* ***Tokenization***  \n",
    "La tokenisation est la première étape de la NLP. C'est le processus de fractionnement des chaînes en jetons qui à leur tour sont de petites structures ou unités. La tokenisation implique trois étapes qui consistent à décomposer une phrase complexe en mots, à comprendre l'importance de chaque mot par rapport à la phrase et enfin à produire une description structurelle sur une phrase d'entrée.  \n",
    "   \n",
    "***Code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFd7uTHddugD",
    "outputId": "908a922e-cf8b-4dd6-a8fe-dfe338930fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Brazil',\n",
       " 'they',\n",
       " 'drive',\n",
       " 'on',\n",
       " 'the',\n",
       " 'right-hand',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'road',\n",
       " '.',\n",
       " 'Brazil',\n",
       " 'has',\n",
       " 'a',\n",
       " 'large',\n",
       " 'coastline',\n",
       " 'on',\n",
       " 'the',\n",
       " 'eastern',\n",
       " 'side',\n",
       " 'of',\n",
       " 'South',\n",
       " 'America']"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# sample text for performing tokenization\n",
    "text = \"In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern side of South America\"\n",
    "\n",
    "# importing word_tokenize from nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Passing the string text into word tokenize for breaking the sentences\n",
    "token = word_tokenize(text)\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ-U5e3egrAV"
   },
   "source": [
    "À partir de la sortie ci-dessus, nous pouvons voir le texte divisé en jetons. Les mots, les virgules, les ponctuations sont appelés des jetons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJDP2Aeugvjc"
   },
   "source": [
    "***Finding frequency distinct in the text***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKyzUgmpf4m6",
    "outputId": "074a2e14-ed24-4352-b646-4e818189d661"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 1,\n",
       "          'America': 1,\n",
       "          'Brazil': 2,\n",
       "          'In': 1,\n",
       "          'South': 1,\n",
       "          'a': 1,\n",
       "          'coastline': 1,\n",
       "          'drive': 1,\n",
       "          'eastern': 1,\n",
       "          'has': 1,\n",
       "          'large': 1,\n",
       "          'of': 2,\n",
       "          'on': 2,\n",
       "          'right-hand': 1,\n",
       "          'road': 1,\n",
       "          'side': 2,\n",
       "          'the': 3,\n",
       "          'they': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(token)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyyZPFqehLKx"
   },
   "source": [
    "'the' se trouve 3 fois dans le texte, 'Brazil' se trouve 2 fois dans le texte, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyv50J0wg89z",
    "outputId": "af063e59-b403-431c-e437-01045c6cc521"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3),\n",
       " ('Brazil', 2),\n",
       " ('on', 2),\n",
       " ('side', 2),\n",
       " ('of', 2),\n",
       " ('In', 1),\n",
       " ('they', 1),\n",
       " ('drive', 1),\n",
       " ('right-hand', 1),\n",
       " ('road', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find the frequency of top 10 words\n",
    "fdist1 = fdist.most_common(10)\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WryAcx-lheqf"
   },
   "source": [
    "* ***Stemming***(Tige)  \n",
    "La tige se réfère généralement à la normalisation des mots dans sa forme de base ou sa forme racine.  \n",
    "   \n",
    "Par exemple, si nous avons des mots attendus, attendant et attendons. Ici, le mot racine est «attendre».Il existe deux méthodes dans Stemming, à savoir, Porter Stemming (supprime les terminaisons morphologiques et flexionnelles communes des mots) et Lancaster Stemming (un algorithme de souche plus agressif)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "shLFsVoghZPQ",
    "outputId": "12fdfba9-f620-43ab-c339-ba57c09d0c17"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'wait'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Porterstemmer from nltk library\n",
    "# Checking for the word ‘giving’ \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pst = PorterStemmer()\n",
    "pst.stem(\"waiting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRqu2fcliDQI",
    "outputId": "7cc55fc0-0d6c-4430-f9a2-233dd23c0157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waited:wait\n",
      "waiting:wait\n",
      "waits:wait\n"
     ]
    }
   ],
   "source": [
    "# Checking for the list of words\n",
    "\n",
    "stm = [\"waited\", \"waiting\", \"waits\"]\n",
    "for word in stm :\n",
    "   print(word+ \":\" +pst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pK1JtDXziNMK",
    "outputId": "c132baa9-6ad7-41c5-8cb9-eb719807384c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving:giv\n",
      "given:giv\n",
      "given:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "# Importing LancasterStemmer from nltk\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lst = LancasterStemmer()\n",
    "stm = [\"giving\", \"given\", \"given\", \"gave\"]\n",
    "for word in stm :\n",
    " print(word+ \":\" +lst.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-MvT0iBi-ow"
   },
   "source": [
    "Lancaster est plus agressif que Porter Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cz-wDXPTjSkB"
   },
   "source": [
    "En termes plus simples, c'est le processus de conversion d'un mot dans sa forme de base. La différence entre la racine(stemming) et la lemmatisation est que la lemmatisation considère le contexte et convertit le mot en sa forme de base significative, alors que la racine(stemming) supprime simplement les derniers caractères, ce qui conduit souvent à des significations incorrectes et des fautes d'orthographe.  \n",
    "   \n",
    "Par exemple, la lemmatisation identifierait correctement la forme de base du «caring» au «care», alors que la tige(stemming) couperait la partie «ing» et la convertirait en voiture.   \n",
    "   \n",
    "La lemmatisation peut être implémentée en python en utilisant Wordnet Lemmatizer, Spacy Lemmatizer, TextBlob, Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7etOQoximsH",
    "outputId": "4b64727d-b379-4687-e097-a368d4dcd1ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "rocks : rock\n",
      "corpora : corpus\n"
     ]
    }
   ],
   "source": [
    "# Importing Lemmatizer library from nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lg5Dtmedkpy-"
   },
   "source": [
    "* ***Stop Words***(Arrêter les mots)  \n",
    " Les «mots vides( “Stop words” ) sont les mots les plus courants dans une langue comme «le», «a», «at», «for», «above», «on», «is», «all». Ces mots n'ont aucun sens et sont généralement supprimés des textes. Nous pouvons supprimer ces mots vides en utilisant la bibliothèque nltk  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5kR7wrPkQo3",
    "outputId": "cfdaf170-13d4-4799-bb9f-ed1dc64480fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "['cristiano', 'ronaldo', 'was', 'born', 'on', 'february', '5', ',', '1985', ',', 'in', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n",
      "['cristiano', 'ronaldo', 'born', 'february', '5', ',', '1985', ',', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n"
     ]
    }
   ],
   "source": [
    "# importing stopwors from nltk library\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "a = set(stopwords.words(\"english\"))\n",
    "\n",
    "text = \"Cristiano Ronaldo was born on February 5, 1985, in Funchal, Madeira, Portugal.\"\n",
    "text1 = word_tokenize(text.lower())\n",
    "print(text1)\n",
    "stopwords = [x for x in text1 if x not in a]\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2zScGKil7_c"
   },
   "source": [
    "* ***Part of speech tagging (POS)*** (Partie du balisage vocal (POS))  \n",
    "   \n",
    "Le balisage de partie de discours(Part-of-speech tagging) est utilisé pour attribuer des parties de discours à chaque mot d'un texte donné (comme les noms, les verbes, les pronoms, les adverbes, la conjonction, les adjectifs, l'interjection) en fonction de sa définition et de son contexte. Il existe de nombreux outils disponibles pour les étiqueteurs de point de vente et certains des étiqueteurs largement utilisés sont NLTK, Spacy, TextBlob, Standford CoreNLP, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOZ0_zbzlkaW",
    "outputId": "5789748d-c33b-4ba2-c3da-b97db13bb6c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[('vote', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('choose', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('particular', 'JJ')]\n",
      "[('man', 'NN')]\n",
      "[('or', 'CC')]\n",
      "[('a', 'DT')]\n",
      "[('group', 'NN')]\n",
      "[('(', '(')]\n",
      "[('party', 'NN')]\n",
      "[(')', ')')]\n",
      "[('to', 'TO')]\n",
      "[('represent', 'NN')]\n",
      "[('them', 'PRP')]\n",
      "[('in', 'IN')]\n",
      "[('parliament', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"vote to choose a particular man or a group (party) to represent them in parliament\"\n",
    "#Tokenize the text\n",
    "tex = word_tokenize(text)\n",
    "for token in tex:\n",
    "  print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGO3dX2NnN-R"
   },
   "source": [
    "* ***Named entity recognition*** (Reconnaissance des entités nommées)  \n",
    "C'est le processus de détection des entités nommées telles que le nom de la personne, le nom de l'emplacement, le nom de l'entreprise, les quantités et la valeur monétaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "1jYaR3sBmnnw",
    "outputId": "ec45d926-3eae-491e-f68e-f7566d10c64b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0m_canvas_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mwidget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_to_treesegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/draw/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent, **kw)\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# If no parent was given, set up a top-level window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLTK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<Control-p>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Google', 'NNP')]), ('’', 'NNP'), ('s', 'VBD'), Tree('ORGANIZATION', [('CEO', 'NNP'), ('Sundar', 'NNP'), ('Pichai', 'NNP')]), ('introduced', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('Pixel', 'NNP'), ('at', 'IN'), Tree('ORGANIZATION', [('Minnesota', 'NNP'), ('Roi', 'NNP'), ('Centre', 'NNP')]), ('Event', 'NNP')])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Google’s CEO Sundar Pichai introduced the new Pixel at Minnesota Roi Centre Event\"\n",
    "\n",
    "#importing chunk library from nltk\n",
    "from nltk import ne_chunk\n",
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "\n",
    "# tokenize and POS Tagging before doing chunk\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "chunk = ne_chunk(tags)\n",
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9ZJaveNotWT"
   },
   "source": [
    "* ***Chunking*** (Regrouper)  \n",
    "La segmentation(Chunking) signifie collecter des informations individuelles et les regrouper en plus grandes parties. Dans le contexte de la NLP et de l'exploration de texte, le segmentation(Chunking) signifie un regroupement de mots ou de jetons en morceaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLn6DA1xoDFb",
    "outputId": "4bf40a77-8d32-407e-bed4-60d2d02aca8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "text = \"We saw the yellow dog\"\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "reg = \"NP: {<DT>?<JJ>*<NN>}\" \n",
    "a = nltk.RegexpParser(reg)\n",
    "result = a.parse(tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PHy81LhpYd7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text_Mining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
